{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing & Debugging (v0.14.0)\n",
    "\n",
    "This notebook demonstrates the testing and debugging utilities in SurrealDB-ORM: **fixtures** for declarative test data, **model factories** for random data generation, and **query logging** for profiling and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Writing Tests for Your ORM Application\n",
    "\n",
    "Good tests need:\n",
    "1. **Repeatable test data** -- fixtures that set up and tear down cleanly\n",
    "2. **Random data generation** -- factories for property-based testing and load testing\n",
    "3. **Query visibility** -- see exactly what SQL hits the database and how long it takes\n",
    "\n",
    "These tools make your test suite faster to write, more reliable, and easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- SurrealDB running locally (`docker run --rm -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root`)\n",
    "- Project dependencies installed (`uv sync`)\n",
    "- A `.env` file in the project root (optional, falls back to defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: add project root to path and configure SurrealDB connection\nimport os, sys\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\nsys.path.append(project_root)\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom src.surreal_orm import SurrealDBConnectionManager\n\nSurrealDBConnectionManager.set_connection(\n    os.getenv(\"SURREALDB_URL\", \"ws://localhost:8000\"),\n    os.getenv(\"SURREALDB_USER\", \"root\"),\n    os.getenv(\"SURREALDB_PASS\", \"root\"),\n    os.getenv(\"SURREALDB_NAMESPACE\", \"ns\"),\n    os.getenv(\"SURREALDB_DATABASE\", \"db\"),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a model we'll use throughout this notebook\nfrom src.surreal_orm import BaseSurrealModel, SurrealConfigDict\n\nclass TestUser(BaseSurrealModel):\n    model_config = SurrealConfigDict(table_name=\"test_user\")\n\n    id: str | None = None\n    name: str\n    role: str = \"user\"\n    score: int = 0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Fixtures -- Declarative Test Data\n",
    "\n",
    "Fixtures let you declare test data as class attributes. When loaded, they save all records to the database. When the context manager exits, they clean up automatically.\n",
    "\n",
    "**When to use fixtures:**\n",
    "- Integration tests that need known data in the database\n",
    "- Setup/teardown patterns where cleanup must be guaranteed\n",
    "- Sharing test data definitions across multiple test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fixtures as a class with model instances as attributes\n",
    "from src.surreal_orm.testing import SurrealFixture, fixture\n",
    "\n",
    "@fixture\n",
    "class UserFixtures(SurrealFixture):\n",
    "    alice = TestUser(name=\"Alice\", role=\"admin\", score=100)\n",
    "    bob = TestUser(name=\"Bob\", role=\"player\", score=50)\n",
    "\n",
    "print(\"Fixtures defined: alice (admin), bob (player)\")\n",
    "print(\"They haven't been saved to the database yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fixtures into the database using async context manager\n",
    "# On enter: saves all fixture records\n",
    "# On exit: deletes all fixture records (guaranteed cleanup)\n",
    "async with UserFixtures.load() as fixtures:\n",
    "    print(f\"Alice ID: {fixtures.alice.get_id()}\")\n",
    "    print(f\"Bob ID: {fixtures.bob.get_id()}\")\n",
    "\n",
    "    # Query the database -- fixtures are real records\n",
    "    users = await TestUser.objects().all()\n",
    "    print(f\"\\nTotal users in DB: {len(users)}\")\n",
    "    for u in users:\n",
    "        print(f\"  - {u.name} ({u.role}), score: {u.score}\")\n",
    "\n",
    "# After the context manager exits, fixtures are cleaned up\n",
    "remaining = await TestUser.objects().all()\n",
    "print(f\"\\nAfter cleanup: {len(remaining)} users in DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Factories -- Random Data Generation\n",
    "\n",
    "Factories generate model instances with random but realistic data. Use `build()` for in-memory instances (unit tests) or `create()` to save to the database (integration tests).\n",
    "\n",
    "**When to use factories:**\n",
    "- Property-based testing with random inputs\n",
    "- Load testing with large datasets\n",
    "- When you need many records but don't care about specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a factory with Faker providers for random data\n",
    "from src.surreal_orm.testing import ModelFactory, Faker\n",
    "\n",
    "class UserFactory(ModelFactory):\n",
    "    class Meta:\n",
    "        model = TestUser\n",
    "\n",
    "    name = Faker(\"name\")\n",
    "    role = Faker(\"choice\", items=[\"admin\", \"player\", \"viewer\"])\n",
    "    score = Faker(\"random_int\", min=0, max=1000)\n",
    "\n",
    "print(\"UserFactory defined with random name, role, and score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build() creates instances WITHOUT saving to the database\n",
    "# Perfect for unit tests where you don't need DB state\n",
    "user = UserFactory.build()\n",
    "print(f\"Built (not saved): {user.name} ({user.role}), score: {user.score}\")\n",
    "print(f\"ID is None (not persisted): {user.id}\")\n",
    "\n",
    "# build_batch() creates multiple instances at once\n",
    "users = UserFactory.build_batch(5)\n",
    "print(f\"\\nBuilt {len(users)} users:\")\n",
    "for u in users:\n",
    "    print(f\"  - {u.name}: {u.role}, score: {u.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override specific fields while keeping others random\n",
    "# Useful for testing specific scenarios\n",
    "admin = UserFactory.build(role=\"admin\", score=999)\n",
    "print(f\"Override example: {admin.name} is {admin.role} with score {admin.score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Factory -- create() and create_batch()\n",
    "\n",
    "Use `create()` and `create_batch()` when you need the records to actually exist in the database (integration tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create() builds AND saves to the database in one call\n",
    "user = await UserFactory.create(role=\"admin\")\n",
    "print(f\"Created admin: {user.name}, ID: {user.get_id()}, score: {user.score}\")\n",
    "\n",
    "# create_batch() creates and saves multiple records\n",
    "players = await UserFactory.create_batch(3, role=\"player\")\n",
    "print(f\"\\nCreated {len(players)} players:\")\n",
    "for p in players:\n",
    "    print(f\"  - {p.name} (ID: {p.get_id()}), score: {p.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the records exist in the database\n",
    "all_users = await TestUser.objects().all()\n",
    "print(f\"Total users in DB: {len(all_users)}\")\n",
    "\n",
    "# Cleanup the factory-created records\n",
    "for u in [user, *players]:\n",
    "    await u.delete()\n",
    "print(\"Factory records cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Faker Providers Reference\n",
    "\n",
    "The `Faker` class provides various data generators. Each call to `.generate()` produces a new random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate all available Faker providers\n",
    "from src.surreal_orm.testing import Faker\n",
    "\n",
    "providers = [\n",
    "    (\"name\", {}),\n",
    "    (\"first_name\", {}),\n",
    "    (\"last_name\", {}),\n",
    "    (\"email\", {}),\n",
    "    (\"random_int\", {\"min\": 1, \"max\": 100}),\n",
    "    (\"random_float\", {\"min\": 0.0, \"max\": 1.0}),\n",
    "    (\"text\", {\"max_length\": 50}),\n",
    "    (\"sentence\", {}),\n",
    "    (\"word\", {}),\n",
    "    (\"uuid\", {}),\n",
    "    (\"boolean\", {}),\n",
    "    (\"date\", {}),\n",
    "    (\"datetime\", {}),\n",
    "    (\"choice\", {\"items\": [\"a\", \"b\", \"c\"]}),\n",
    "]\n",
    "\n",
    "print(\"Available Faker providers:\")\n",
    "for name, kwargs in providers:\n",
    "    value = Faker(name, **kwargs).generate()\n",
    "    print(f\"  Faker({name!r}): {value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QueryLogger -- Debug and Profile Queries\n",
    "\n",
    "The `QueryLogger` captures all SurrealQL queries executed within its context, along with timing information. Use it to:\n",
    "\n",
    "- Debug unexpected query patterns (N+1 queries, missing filters)\n",
    "- Profile query performance (find slow queries)\n",
    "- Verify that the ORM generates the SQL you expect\n",
    "\n",
    "The logger uses `contextvars`, so it's async-safe and only captures queries from the current async context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test data for query logging\n",
    "test_users = await UserFactory.create_batch(3)\n",
    "print(f\"Created {len(test_users)} test users for profiling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QueryLogger captures all queries within the async with block\n",
    "from src.surreal_orm.debug import QueryLogger\n",
    "\n",
    "async with QueryLogger() as logger:\n",
    "    # These queries will be captured\n",
    "    users = await TestUser.objects().all()\n",
    "    user = UserFactory.build(name=\"LoggerTest\")\n",
    "    await user.save()\n",
    "    await user.delete()\n",
    "\n",
    "# After the block, inspect the captured queries\n",
    "print(f\"Total: {logger.total_queries} queries in {logger.total_ms:.1f}ms\")\n",
    "print(\"\\nCaptured queries:\")\n",
    "for q in logger.queries:\n",
    "    print(f\"  [{q.duration_ms:.1f}ms] {q.sql}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use QueryLogger to detect N+1 query problems\n",
    "# If you see many similar queries in a loop, you likely need prefetch_related()\n",
    "async with QueryLogger() as logger:\n",
    "    # This is the \"bad\" pattern -- one query per user\n",
    "    users = await TestUser.objects().all()\n",
    "    for u in users:\n",
    "        # Imagine fetching related records one by one here\n",
    "        _ = await TestUser.objects().get(u.get_id())\n",
    "\n",
    "print(f\"N+1 detection: {logger.total_queries} queries for {len(users)} users\")\n",
    "print(\"If this number grows linearly with record count, use prefetch_related()!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QueryLogger Properties\n",
    "\n",
    "| Property | Type | Description |\n",
    "|---|---|---|\n",
    "| `logger.queries` | `list[QueryLog]` | All captured query objects |\n",
    "| `logger.total_queries` | `int` | Number of queries captured |\n",
    "| `logger.total_ms` | `float` | Total execution time in milliseconds |\n",
    "| `q.sql` | `str` | The SurrealQL query string |\n",
    "| `q.duration_ms` | `float` | Execution time for this query |\n",
    "\n",
    "The QueryLogger uses `contextvars` internally, which means it is async-safe. It only captures queries executed within the same async context (the `async with` block). Queries from other concurrent tasks are not captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: remove all test data\n",
    "await TestUser.objects().delete_table()\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}